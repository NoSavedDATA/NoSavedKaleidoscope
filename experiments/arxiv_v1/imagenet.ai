start_timer(0)

class Residual_Block
    def __init__(f in_channels, f out_channels)

        
        Conv2d[in_channels,  out_channels, 3, 1, 1, xavu_relu] self.conv1
        BatchNorm2d[out_channels] self.bn1

        Conv2d[out_channels, out_channels, 3, 1, 1, xavu_relu] self.conv2
        BatchNorm2d[out_channels] self.bn2
    
    def forward(t x)
        tensor[0] z

        z = x
        
        x = self.conv1(x)
        x = self.bn1(x)
        x = relu(x)

        x = self.conv2(x)
        x = self.bn2(x)

        x = relu(x) + z

        return x


class Parametrized_Residual_Block
    def __init__(f in_channels, f out_channels, f stride)


        Conv2d[in_channels,  out_channels, 1, stride, 0, xavu_relu] self.residual
        BatchNorm2d[out_channels] self.residual_bn

        Conv2d[in_channels,  out_channels, 3, stride, 1, xavu_relu] self.conv1
        BatchNorm2d[out_channels] self.bn1

        Conv2d[out_channels, out_channels, 3, 1,      1, xavu_relu] self.conv2
        BatchNorm2d[out_channels] self.bn2
    
    def forward(t x)
        tensor[0] z

        z = self.residual(x)
        z = self.residual_bn(z)
        
        x = self.conv1(x)
        x = self.bn1(x)
        x = relu(x)
        
        x = self.conv2(x)
        x = self.bn2(x)

        x = relu(x) + z

        return x


class Model
	def __init__()
        
        Conv2d[3,64,7,2,3,xavu_relu] self.conv1
        MaxPool2d[3,2,1] self.pool1
        BatchNorm2d[64] self.bn1



        Residual_Block self.residual_1
        self.residual_1.__init__(64, 64)
        Residual_Block self.residual_2
        self.residual_2.__init__(64, 64)


		Parametrized_Residual_Block self.residual_3
        self.residual_3.__init__(64,  128, 2)
        Residual_Block self.residual_4
        self.residual_4.__init__(128, 128)
        

        Parametrized_Residual_Block self.residual_5
        self.residual_5.__init__(128, 256, 2)
        Residual_Block self.residual_6
        self.residual_6.__init__(256, 256)


        Parametrized_Residual_Block self.residual_7
        self.residual_7.__init__(256, 512, 2)
        Residual_Block self.residual_8
        self.residual_8.__init__(512, 512)


        AvgPool2d[7,1,0] self.pool_out
		param[1000, 512, xavu] self.w1

        
	def forward(t x)
		x = self.conv1(x)
        x = self.pool1(x)
        x = self.bn1(x)
        x = relu(x)

        x = self.residual_1.forward(x)
        x = self.residual_2.forward(x)

        x = self.residual_3.forward(x)
        x = self.residual_4.forward(x)

        x = self.residual_5.forward(x)
        x = self.residual_6.forward(x)
        
        x = self.residual_7.forward(x)
        x = self.residual_8.forward(x)


        x = self.pool_out(x)

        x = x.view(-1, 512)

        x = x@self.w1


		return x

class Dataset
    def __init__(f batch_size, f num_workers, s path)
        self.batch_size = batch_size
        self.num_workers = num_workers

        float _len, yield_ptr, running_workers, all_yielded
        self._len = 0
        self.yield_ptr = 0
        self.all_yielded = 0
        self.running_workers = 1

        
        float_vec can_load_, can_process_
        self.can_load_ = zeros_vec(num_workers)
        self.can_process_ = ones_vec(num_workers)

        str_vec files
        self.files = glob(path)

        pinned_tensor[num_workers, batch_size, 3,224,224] self.p_x
        pinned_tensor[num_workers, batch_size] self.p_y

        tensor[0] self.mean
        tensor[0] self.std

        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]
        


    def len()
        self._len  = LenStrVec(self.files)
        self.files = ShuffleStrVec(self.files)
        self._len


    def terminate_workers()
        print("Terminating workers", 0)
        self.running_workers = 0
        sleep(1)
        0


    def increment_yield_ptr()
        lock "yield"
            if (self.yield_ptr + self.batch_size*(self.num_workers+2)) > self._len
                self.yield_ptr = 0
                self.files = ShuffleStrVec(self.files)
                0
            else
                self.yield_ptr = self.yield_ptr + self.batch_size
        self.yield_ptr


    def getitem_w(f idx, f b, f w)
        str aux

        aux = self.files[idx]
        
        wload_img_resize(self.p_x, aux, w, b, 3, 224, 224)
        

        self.p_y[w,b] = to_float(aux.split_idx("/", -2))
        
        0


    def worker(f w)

        float bs, can_process, yield_ptr
        bs = self.batch_size


        while self.running_workers == 1
            
            can_process = self.can_process_[w]
            #print("worker", w)
            #print("can_process:", can_process)
            if 1==can_process

                self.can_load_[w] = 0
                self.can_process_[w] = 0

                yield_ptr = self.increment_yield_ptr()
                

                for b=0, b < bs
                    self.getitem_w(yield_ptr+b, b, w)
                    


                self.can_load_[w] = 1
                
                0


    def attr_w(t x, t y)

        float w = -1

        while w == -1
            w = self.can_load_.first_nonzero()
        
        #print("ATTRIBUTING", w)

        self.can_process_[w] = 0
        self.can_load_[w] = 0
        
        
        x.gpuw(self.p_x, w)
        y.gpuw(self.p_y, w)


        x = RandomHorizontalFlip(x)
        x = RandomCrop(x, 4)
        #x.save_img("flipped")
        x = NormalizeImg(x, self.mean, self.std)
        
        
        self.can_process_[w] = 1

        return x, y

    def attr_eval(t x, t y)

        float w = -1

        while w == -1
            w = self.can_load_.first_nonzero()
        
        #print("ATTRIBUTING", w)

        self.can_process_[w] = 0
        self.can_load_[w] = 0
        
        
        x.gpuw(self.p_x, w)
        y.gpuw(self.p_y, w)

        
        x = NormalizeImg(x, self.mean, self.std)
        
        
        
        self.can_process_[w] = 1

        return x, y
        


Model model
model.__init__();



float max_lr, lr, max_steps, batch_size, steps_per_epoch, epoch

batch_size = 256


epoch = 0
lr = 0.1
max_lr = 0.1


steps_per_epoch = round(1281167/batch_size)
max_steps = 10000

Dataset dataset
dataset.__init__(batch_size, 12, "/mnt/d/datasets/ImageNet/train/*/*.JPEG");
dataset.len();



finish

    async dataset.worker(0)
    async dataset.worker(1)
    async dataset.worker(2)
    async dataset.worker(3)
    async dataset.worker(4)
    async dataset.worker(5)
    async dataset.worker(6)
    async dataset.worker(7)
    async dataset.worker(8)
    async dataset.worker(9)
    async dataset.worker(10)
    async dataset.worker(11)
    #async dataset.worker(12)
    #async dataset.worker(13)



    sleep(2)

    
    for i=0, i<max_steps

        tensor[0] x
        tensor[0] y


        dataset.attr_w(x, y)


        model.forward(x)
        y = y.onehot(1000)


        cross_entropy(x, y, 1/(batch_size*1000))
        backprop()

        if (i%steps_per_epoch==0)
            print("Epoch", epoch)
            epoch = epoch + 1
            
            print("lr", lr)


        lr = CosineLR(max_lr, 0, i, max_steps)

        SGD(lr, 0.9, 0.0001, 10)

        print("i",i)
        
        0
        

    dataset.terminate_workers()

dataset.all_yielded;



eval()


float val_steps, val_batch_size

val_batch_size = 50


Dataset val_dataset
val_dataset.__init__(val_batch_size, 3, "/mnt/d/datasets/ImageNet/val/*/*.JPEG");

val_steps = val_dataset.len();
val_steps = round(val_steps/val_batch_size)

print("Val steps post", val_steps)



tensor[1,zeros] acc

finish

    async val_dataset.worker(0)
    async val_dataset.worker(1)
    async val_dataset.worker(2)
    
    for i=0, i<val_steps
        print("i:", i)
        tensor[0] x
        tensor[0] y
        tensor[1] batch_acc

        val_dataset.attr_eval(x, y)


        model.forward(x)

        x = x.argmax(-1)
        
        
        
        batch_acc = x==y
        acc = acc + batch_acc.mean()
        clean_forward()

    val_dataset.terminate_workers()


acc = 100*acc/val_steps

print("Accuracy",0)
acc;




end_timer(0);